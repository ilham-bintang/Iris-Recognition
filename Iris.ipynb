{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd        # For loading and processing the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  Species\n",
       "0   1            5.1           3.5            1.4           0.2        0\n",
       "1   2            4.9           3.0            1.4           0.2        0\n",
       "2   3            4.7           3.2            1.3           0.2        0\n",
       "3   4            4.6           3.1            1.5           0.2        0\n",
       "4   5            5.0           3.6            1.4           0.2        0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"Iris.csv\"\n",
    "# load dataset into Pandas DataFrame\n",
    "\n",
    "features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\n",
    "df = pd.read_csv(url)\n",
    "df.Species = pd.factorize(df.Species)[0]\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating out the features\n",
    "X = df.loc[:, features].values\n",
    "\n",
    "# Separating out the target\n",
    "y = df.loc[:,['Species']].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 4)\n",
      "(38, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "iris = df\n",
    "iris[\"Id\"] = iris.index\n",
    "iris[\"ratio_sepal\"] = iris[\"SepalLengthCm\"]/iris[\"SepalWidthCm\"]\n",
    "iris[\"ratio_petal\"] = iris[\"PetalLengthCm\"]/iris[\"PetalWidthCm\"]\n",
    "\n",
    "# sns.lmplot(x=\"Id\", y=\"ratio_sepal\", data=iris, hue=\"Species\", fit_reg=False, legend=False)\n",
    "\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "sns.lmplot(x=\"ratio_sepal\", y=\"ratio_petal\", data=iris, hue=\"Species\", fit_reg=False, legend=False)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10, 8),\n",
    "                    max_iter=1000, \n",
    "                    verbose=True, \n",
    "                    learning_rate_init=0.001,\n",
    "                    solver='adam',\n",
    "                    alpha=0.0001,\n",
    "                    activation='tanh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nullphantom/miniconda3/envs/ml/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:921: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.74475101\n",
      "Iteration 2, loss = 1.72227119\n",
      "Iteration 3, loss = 1.70016751\n",
      "Iteration 4, loss = 1.67845072\n",
      "Iteration 5, loss = 1.65712944\n",
      "Iteration 6, loss = 1.63621463\n",
      "Iteration 7, loss = 1.61571685\n",
      "Iteration 8, loss = 1.59564626\n",
      "Iteration 9, loss = 1.57601204\n",
      "Iteration 10, loss = 1.55682160\n",
      "Iteration 11, loss = 1.53808057\n",
      "Iteration 12, loss = 1.51979294\n",
      "Iteration 13, loss = 1.50196110\n",
      "Iteration 14, loss = 1.48458580\n",
      "Iteration 15, loss = 1.46766601\n",
      "Iteration 16, loss = 1.45119894\n",
      "Iteration 17, loss = 1.43518006\n",
      "Iteration 18, loss = 1.41960323\n",
      "Iteration 19, loss = 1.40446081\n",
      "Iteration 20, loss = 1.38974385\n",
      "Iteration 21, loss = 1.37544217\n",
      "Iteration 22, loss = 1.36154460\n",
      "Iteration 23, loss = 1.34803903\n",
      "Iteration 24, loss = 1.33491267\n",
      "Iteration 25, loss = 1.32215210\n",
      "Iteration 26, loss = 1.30974354\n",
      "Iteration 27, loss = 1.29767292\n",
      "Iteration 28, loss = 1.28592615\n",
      "Iteration 29, loss = 1.27448917\n",
      "Iteration 30, loss = 1.26334819\n",
      "Iteration 31, loss = 1.25248977\n",
      "Iteration 32, loss = 1.24190093\n",
      "Iteration 33, loss = 1.23156929\n",
      "Iteration 34, loss = 1.22148305\n",
      "Iteration 35, loss = 1.21163115\n",
      "Iteration 36, loss = 1.20200317\n",
      "Iteration 37, loss = 1.19258946\n",
      "Iteration 38, loss = 1.18338104\n",
      "Iteration 39, loss = 1.17436965\n",
      "Iteration 40, loss = 1.16554768\n",
      "Iteration 41, loss = 1.15690816\n",
      "Iteration 42, loss = 1.14844471\n",
      "Iteration 43, loss = 1.14015151\n",
      "Iteration 44, loss = 1.13202327\n",
      "Iteration 45, loss = 1.12405516\n",
      "Iteration 46, loss = 1.11624280\n",
      "Iteration 47, loss = 1.10858221\n",
      "Iteration 48, loss = 1.10106976\n",
      "Iteration 49, loss = 1.09370214\n",
      "Iteration 50, loss = 1.08647631\n",
      "Iteration 51, loss = 1.07938946\n",
      "Iteration 52, loss = 1.07243897\n",
      "Iteration 53, loss = 1.06562237\n",
      "Iteration 54, loss = 1.05893729\n",
      "Iteration 55, loss = 1.05238144\n",
      "Iteration 56, loss = 1.04595262\n",
      "Iteration 57, loss = 1.03964861\n",
      "Iteration 58, loss = 1.03346722\n",
      "Iteration 59, loss = 1.02740628\n",
      "Iteration 60, loss = 1.02146356\n",
      "Iteration 61, loss = 1.01563681\n",
      "Iteration 62, loss = 1.00992375\n",
      "Iteration 63, loss = 1.00432205\n",
      "Iteration 64, loss = 0.99882932\n",
      "Iteration 65, loss = 0.99344311\n",
      "Iteration 66, loss = 0.98816093\n",
      "Iteration 67, loss = 0.98298020\n",
      "Iteration 68, loss = 0.97789832\n",
      "Iteration 69, loss = 0.97291259\n",
      "Iteration 70, loss = 0.96802027\n",
      "Iteration 71, loss = 0.96321858\n",
      "Iteration 72, loss = 0.95850465\n",
      "Iteration 73, loss = 0.95387561\n",
      "Iteration 74, loss = 0.94932851\n",
      "Iteration 75, loss = 0.94486038\n",
      "Iteration 76, loss = 0.94046822\n",
      "Iteration 77, loss = 0.93614900\n",
      "Iteration 78, loss = 0.93189969\n",
      "Iteration 79, loss = 0.92771724\n",
      "Iteration 80, loss = 0.92359861\n",
      "Iteration 81, loss = 0.91954075\n",
      "Iteration 82, loss = 0.91554066\n",
      "Iteration 83, loss = 0.91159532\n",
      "Iteration 84, loss = 0.90770178\n",
      "Iteration 85, loss = 0.90385709\n",
      "Iteration 86, loss = 0.90005834\n",
      "Iteration 87, loss = 0.89630266\n",
      "Iteration 88, loss = 0.89258722\n",
      "Iteration 89, loss = 0.88890920\n",
      "Iteration 90, loss = 0.88526582\n",
      "Iteration 91, loss = 0.88165430\n",
      "Iteration 92, loss = 0.87807189\n",
      "Iteration 93, loss = 0.87451585\n",
      "Iteration 94, loss = 0.87098342\n",
      "Iteration 95, loss = 0.86747184\n",
      "Iteration 96, loss = 0.86397836\n",
      "Iteration 97, loss = 0.86050023\n",
      "Iteration 98, loss = 0.85703470\n",
      "Iteration 99, loss = 0.85357909\n",
      "Iteration 100, loss = 0.85013077\n",
      "Iteration 101, loss = 0.84668719\n",
      "Iteration 102, loss = 0.84324597\n",
      "Iteration 103, loss = 0.83980485\n",
      "Iteration 104, loss = 0.83636182\n",
      "Iteration 105, loss = 0.83291508\n",
      "Iteration 106, loss = 0.82946310\n",
      "Iteration 107, loss = 0.82600464\n",
      "Iteration 108, loss = 0.82253877\n",
      "Iteration 109, loss = 0.81906486\n",
      "Iteration 110, loss = 0.81558263\n",
      "Iteration 111, loss = 0.81209212\n",
      "Iteration 112, loss = 0.80859368\n",
      "Iteration 113, loss = 0.80508801\n",
      "Iteration 114, loss = 0.80157612\n",
      "Iteration 115, loss = 0.79805930\n",
      "Iteration 116, loss = 0.79453917\n",
      "Iteration 117, loss = 0.79101758\n",
      "Iteration 118, loss = 0.78749664\n",
      "Iteration 119, loss = 0.78397866\n",
      "Iteration 120, loss = 0.78046616\n",
      "Iteration 121, loss = 0.77696178\n",
      "Iteration 122, loss = 0.77346831\n",
      "Iteration 123, loss = 0.76998859\n",
      "Iteration 124, loss = 0.76652553\n",
      "Iteration 125, loss = 0.76308201\n",
      "Iteration 126, loss = 0.75966092\n",
      "Iteration 127, loss = 0.75626504\n",
      "Iteration 128, loss = 0.75289706\n",
      "Iteration 129, loss = 0.74955950\n",
      "Iteration 130, loss = 0.74625469\n",
      "Iteration 131, loss = 0.74298471\n",
      "Iteration 132, loss = 0.73975135\n",
      "Iteration 133, loss = 0.73655612\n",
      "Iteration 134, loss = 0.73340017\n",
      "Iteration 135, loss = 0.73028433\n",
      "Iteration 136, loss = 0.72720907\n",
      "Iteration 137, loss = 0.72417458\n",
      "Iteration 138, loss = 0.72118076\n",
      "Iteration 139, loss = 0.71822732\n",
      "Iteration 140, loss = 0.71531376\n",
      "Iteration 141, loss = 0.71243949\n",
      "Iteration 142, loss = 0.70960382\n",
      "Iteration 143, loss = 0.70680600\n",
      "Iteration 144, loss = 0.70404524\n",
      "Iteration 145, loss = 0.70132072\n",
      "Iteration 146, loss = 0.69863155\n",
      "Iteration 147, loss = 0.69597685\n",
      "Iteration 148, loss = 0.69335571\n",
      "Iteration 149, loss = 0.69076718\n",
      "Iteration 150, loss = 0.68821031\n",
      "Iteration 151, loss = 0.68568416\n",
      "Iteration 152, loss = 0.68318779\n",
      "Iteration 153, loss = 0.68072029\n",
      "Iteration 154, loss = 0.67828077\n",
      "Iteration 155, loss = 0.67586840\n",
      "Iteration 156, loss = 0.67348237\n",
      "Iteration 157, loss = 0.67112195\n",
      "Iteration 158, loss = 0.66878644\n",
      "Iteration 159, loss = 0.66647521\n",
      "Iteration 160, loss = 0.66418768\n",
      "Iteration 161, loss = 0.66192332\n",
      "Iteration 162, loss = 0.65968167\n",
      "Iteration 163, loss = 0.65746229\n",
      "Iteration 164, loss = 0.65526479\n",
      "Iteration 165, loss = 0.65308885\n",
      "Iteration 166, loss = 0.65093414\n",
      "Iteration 167, loss = 0.64880040\n",
      "Iteration 168, loss = 0.64668738\n",
      "Iteration 169, loss = 0.64459486\n",
      "Iteration 170, loss = 0.64252264\n",
      "Iteration 171, loss = 0.64047055\n",
      "Iteration 172, loss = 0.63843841\n",
      "Iteration 173, loss = 0.63642607\n",
      "Iteration 174, loss = 0.63443340\n",
      "Iteration 175, loss = 0.63246025\n",
      "Iteration 176, loss = 0.63050649\n",
      "Iteration 177, loss = 0.62857199\n",
      "Iteration 178, loss = 0.62665663\n",
      "Iteration 179, loss = 0.62476027\n",
      "Iteration 180, loss = 0.62288279\n",
      "Iteration 181, loss = 0.62102405\n",
      "Iteration 182, loss = 0.61918392\n",
      "Iteration 183, loss = 0.61736226\n",
      "Iteration 184, loss = 0.61555894\n",
      "Iteration 185, loss = 0.61377382\n",
      "Iteration 186, loss = 0.61200674\n",
      "Iteration 187, loss = 0.61025758\n",
      "Iteration 188, loss = 0.60852618\n",
      "Iteration 189, loss = 0.60681239\n",
      "Iteration 190, loss = 0.60511608\n",
      "Iteration 191, loss = 0.60343711\n",
      "Iteration 192, loss = 0.60177532\n",
      "Iteration 193, loss = 0.60013058\n",
      "Iteration 194, loss = 0.59850275\n",
      "Iteration 195, loss = 0.59689168\n",
      "Iteration 196, loss = 0.59529724\n",
      "Iteration 197, loss = 0.59371928\n",
      "Iteration 198, loss = 0.59215767\n",
      "Iteration 199, loss = 0.59061227\n",
      "Iteration 200, loss = 0.58908293\n",
      "Iteration 201, loss = 0.58756951\n",
      "Iteration 202, loss = 0.58607187\n",
      "Iteration 203, loss = 0.58458986\n",
      "Iteration 204, loss = 0.58312335\n",
      "Iteration 205, loss = 0.58167218\n",
      "Iteration 206, loss = 0.58023621\n",
      "Iteration 207, loss = 0.57881529\n",
      "Iteration 208, loss = 0.57740927\n",
      "Iteration 209, loss = 0.57601800\n",
      "Iteration 210, loss = 0.57464132\n",
      "Iteration 211, loss = 0.57327908\n",
      "Iteration 212, loss = 0.57193111\n",
      "Iteration 213, loss = 0.57059727\n",
      "Iteration 214, loss = 0.56927737\n",
      "Iteration 215, loss = 0.56797124\n",
      "Iteration 216, loss = 0.56667872\n",
      "Iteration 217, loss = 0.56539961\n",
      "Iteration 218, loss = 0.56413374\n",
      "Iteration 219, loss = 0.56288091\n",
      "Iteration 220, loss = 0.56164093\n",
      "Iteration 221, loss = 0.56041358\n",
      "Iteration 222, loss = 0.55919867\n",
      "Iteration 223, loss = 0.55799598\n",
      "Iteration 224, loss = 0.55680528\n",
      "Iteration 225, loss = 0.55562634\n",
      "Iteration 226, loss = 0.55445895\n",
      "Iteration 227, loss = 0.55330284\n",
      "Iteration 228, loss = 0.55215778\n",
      "Iteration 229, loss = 0.55102352\n",
      "Iteration 230, loss = 0.54989978\n",
      "Iteration 231, loss = 0.54878630\n",
      "Iteration 232, loss = 0.54768281\n",
      "Iteration 233, loss = 0.54658903\n",
      "Iteration 234, loss = 0.54550466\n",
      "Iteration 235, loss = 0.54442942\n",
      "Iteration 236, loss = 0.54336300\n",
      "Iteration 237, loss = 0.54230511\n",
      "Iteration 238, loss = 0.54125542\n",
      "Iteration 239, loss = 0.54021363\n",
      "Iteration 240, loss = 0.53917942\n",
      "Iteration 241, loss = 0.53815247\n",
      "Iteration 242, loss = 0.53713245\n",
      "Iteration 243, loss = 0.53611903\n",
      "Iteration 244, loss = 0.53511189\n",
      "Iteration 245, loss = 0.53411067\n",
      "Iteration 246, loss = 0.53311505\n",
      "Iteration 247, loss = 0.53212469\n",
      "Iteration 248, loss = 0.53113925\n",
      "Iteration 249, loss = 0.53015837\n",
      "Iteration 250, loss = 0.52918172\n",
      "Iteration 251, loss = 0.52820895\n",
      "Iteration 252, loss = 0.52723971\n",
      "Iteration 253, loss = 0.52627365\n",
      "Iteration 254, loss = 0.52531043\n",
      "Iteration 255, loss = 0.52434969\n",
      "Iteration 256, loss = 0.52339109\n",
      "Iteration 257, loss = 0.52243426\n",
      "Iteration 258, loss = 0.52147887\n",
      "Iteration 259, loss = 0.52052456\n",
      "Iteration 260, loss = 0.51957097\n",
      "Iteration 261, loss = 0.51861776\n",
      "Iteration 262, loss = 0.51766457\n",
      "Iteration 263, loss = 0.51671105\n",
      "Iteration 264, loss = 0.51575685\n",
      "Iteration 265, loss = 0.51480161\n",
      "Iteration 266, loss = 0.51384498\n",
      "Iteration 267, loss = 0.51288661\n",
      "Iteration 268, loss = 0.51192614\n",
      "Iteration 269, loss = 0.51096322\n",
      "Iteration 270, loss = 0.50999749\n",
      "Iteration 271, loss = 0.50902860\n",
      "Iteration 272, loss = 0.50805620\n",
      "Iteration 273, loss = 0.50707992\n",
      "Iteration 274, loss = 0.50609942\n",
      "Iteration 275, loss = 0.50511432\n",
      "Iteration 276, loss = 0.50412428\n",
      "Iteration 277, loss = 0.50312894\n",
      "Iteration 278, loss = 0.50212793\n",
      "Iteration 279, loss = 0.50112088\n",
      "Iteration 280, loss = 0.50010744\n",
      "Iteration 281, loss = 0.49908724\n",
      "Iteration 282, loss = 0.49805991\n",
      "Iteration 283, loss = 0.49702508\n",
      "Iteration 284, loss = 0.49598238\n",
      "Iteration 285, loss = 0.49493143\n",
      "Iteration 286, loss = 0.49387185\n",
      "Iteration 287, loss = 0.49280327\n",
      "Iteration 288, loss = 0.49172530\n",
      "Iteration 289, loss = 0.49063756\n",
      "Iteration 290, loss = 0.48953967\n",
      "Iteration 291, loss = 0.48843122\n",
      "Iteration 292, loss = 0.48731184\n",
      "Iteration 293, loss = 0.48618112\n",
      "Iteration 294, loss = 0.48503866\n",
      "Iteration 295, loss = 0.48388408\n",
      "Iteration 296, loss = 0.48271695\n",
      "Iteration 297, loss = 0.48153690\n",
      "Iteration 298, loss = 0.48034349\n",
      "Iteration 299, loss = 0.47913634\n",
      "Iteration 300, loss = 0.47791503\n",
      "Iteration 301, loss = 0.47667916\n",
      "Iteration 302, loss = 0.47542833\n",
      "Iteration 303, loss = 0.47416212\n",
      "Iteration 304, loss = 0.47288015\n",
      "Iteration 305, loss = 0.47158200\n",
      "Iteration 306, loss = 0.47026730\n",
      "Iteration 307, loss = 0.46893564\n",
      "Iteration 308, loss = 0.46758666\n",
      "Iteration 309, loss = 0.46621999\n",
      "Iteration 310, loss = 0.46483526\n",
      "Iteration 311, loss = 0.46343212\n",
      "Iteration 312, loss = 0.46201026\n",
      "Iteration 313, loss = 0.46056934\n",
      "Iteration 314, loss = 0.45910908\n",
      "Iteration 315, loss = 0.45762920\n",
      "Iteration 316, loss = 0.45612946\n",
      "Iteration 317, loss = 0.45460961\n",
      "Iteration 318, loss = 0.45306948\n",
      "Iteration 319, loss = 0.45150889\n",
      "Iteration 320, loss = 0.44992771\n",
      "Iteration 321, loss = 0.44832585\n",
      "Iteration 322, loss = 0.44670325\n",
      "Iteration 323, loss = 0.44505989\n",
      "Iteration 324, loss = 0.44339581\n",
      "Iteration 325, loss = 0.44171107\n",
      "Iteration 326, loss = 0.44000578\n",
      "Iteration 327, loss = 0.43828012\n",
      "Iteration 328, loss = 0.43653429\n",
      "Iteration 329, loss = 0.43476855\n",
      "Iteration 330, loss = 0.43298321\n",
      "Iteration 331, loss = 0.43117862\n",
      "Iteration 332, loss = 0.42935518\n",
      "Iteration 333, loss = 0.42751333\n",
      "Iteration 334, loss = 0.42565357\n",
      "Iteration 335, loss = 0.42377642\n",
      "Iteration 336, loss = 0.42188244\n",
      "Iteration 337, loss = 0.41997223\n",
      "Iteration 338, loss = 0.41804642\n",
      "Iteration 339, loss = 0.41610566\n",
      "Iteration 340, loss = 0.41415064\n",
      "Iteration 341, loss = 0.41218203\n",
      "Iteration 342, loss = 0.41020057\n",
      "Iteration 343, loss = 0.40820695\n",
      "Iteration 344, loss = 0.40620192\n",
      "Iteration 345, loss = 0.40418619\n",
      "Iteration 346, loss = 0.40216050\n",
      "Iteration 347, loss = 0.40012557\n",
      "Iteration 348, loss = 0.39808212\n",
      "Iteration 349, loss = 0.39603085\n",
      "Iteration 350, loss = 0.39397245\n",
      "Iteration 351, loss = 0.39190761\n",
      "Iteration 352, loss = 0.38983701\n",
      "Iteration 353, loss = 0.38776128\n",
      "Iteration 354, loss = 0.38568108\n",
      "Iteration 355, loss = 0.38359701\n",
      "Iteration 356, loss = 0.38150970\n",
      "Iteration 357, loss = 0.37941973\n",
      "Iteration 358, loss = 0.37732768\n",
      "Iteration 359, loss = 0.37523411\n",
      "Iteration 360, loss = 0.37313958\n",
      "Iteration 361, loss = 0.37104461\n",
      "Iteration 362, loss = 0.36894975\n",
      "Iteration 363, loss = 0.36685550\n",
      "Iteration 364, loss = 0.36476237\n",
      "Iteration 365, loss = 0.36267086\n",
      "Iteration 366, loss = 0.36058145\n",
      "Iteration 367, loss = 0.35849461\n",
      "Iteration 368, loss = 0.35641082\n",
      "Iteration 369, loss = 0.35433052\n",
      "Iteration 370, loss = 0.35225418\n",
      "Iteration 371, loss = 0.35018222\n",
      "Iteration 372, loss = 0.34811507\n",
      "Iteration 373, loss = 0.34605316\n",
      "Iteration 374, loss = 0.34399688\n",
      "Iteration 375, loss = 0.34194664\n",
      "Iteration 376, loss = 0.33990281\n",
      "Iteration 377, loss = 0.33786577\n",
      "Iteration 378, loss = 0.33583587\n",
      "Iteration 379, loss = 0.33381346\n",
      "Iteration 380, loss = 0.33179887\n",
      "Iteration 381, loss = 0.32979242\n",
      "Iteration 382, loss = 0.32779442\n",
      "Iteration 383, loss = 0.32580515\n",
      "Iteration 384, loss = 0.32382489\n",
      "Iteration 385, loss = 0.32185391\n",
      "Iteration 386, loss = 0.31989246\n",
      "Iteration 387, loss = 0.31794077\n",
      "Iteration 388, loss = 0.31599908\n",
      "Iteration 389, loss = 0.31406758\n",
      "Iteration 390, loss = 0.31214649\n",
      "Iteration 391, loss = 0.31023600\n",
      "Iteration 392, loss = 0.30833628\n",
      "Iteration 393, loss = 0.30644750\n",
      "Iteration 394, loss = 0.30456982\n",
      "Iteration 395, loss = 0.30270339\n",
      "Iteration 396, loss = 0.30084835\n",
      "Iteration 397, loss = 0.29900482\n",
      "Iteration 398, loss = 0.29717293\n",
      "Iteration 399, loss = 0.29535280\n",
      "Iteration 400, loss = 0.29354451\n",
      "Iteration 401, loss = 0.29174819\n",
      "Iteration 402, loss = 0.28996390\n",
      "Iteration 403, loss = 0.28819174\n",
      "Iteration 404, loss = 0.28643177\n",
      "Iteration 405, loss = 0.28468408\n",
      "Iteration 406, loss = 0.28294871\n",
      "Iteration 407, loss = 0.28122572\n",
      "Iteration 408, loss = 0.27951517\n",
      "Iteration 409, loss = 0.27781709\n",
      "Iteration 410, loss = 0.27613152\n",
      "Iteration 411, loss = 0.27445848\n",
      "Iteration 412, loss = 0.27279802\n",
      "Iteration 413, loss = 0.27115013\n",
      "Iteration 414, loss = 0.26951484\n",
      "Iteration 415, loss = 0.26789216\n",
      "Iteration 416, loss = 0.26628208\n",
      "Iteration 417, loss = 0.26468461\n",
      "Iteration 418, loss = 0.26309974\n",
      "Iteration 419, loss = 0.26152745\n",
      "Iteration 420, loss = 0.25996775\n",
      "Iteration 421, loss = 0.25842059\n",
      "Iteration 422, loss = 0.25688597\n",
      "Iteration 423, loss = 0.25536386\n",
      "Iteration 424, loss = 0.25385421\n",
      "Iteration 425, loss = 0.25235701\n",
      "Iteration 426, loss = 0.25087221\n",
      "Iteration 427, loss = 0.24939977\n",
      "Iteration 428, loss = 0.24793965\n",
      "Iteration 429, loss = 0.24649180\n",
      "Iteration 430, loss = 0.24505617\n",
      "Iteration 431, loss = 0.24363270\n",
      "Iteration 432, loss = 0.24222135\n",
      "Iteration 433, loss = 0.24082206\n",
      "Iteration 434, loss = 0.23943476\n",
      "Iteration 435, loss = 0.23805939\n",
      "Iteration 436, loss = 0.23669590\n",
      "Iteration 437, loss = 0.23534421\n",
      "Iteration 438, loss = 0.23400425\n",
      "Iteration 439, loss = 0.23267597\n",
      "Iteration 440, loss = 0.23135927\n",
      "Iteration 441, loss = 0.23005410\n",
      "Iteration 442, loss = 0.22876038\n",
      "Iteration 443, loss = 0.22747803\n",
      "Iteration 444, loss = 0.22620697\n",
      "Iteration 445, loss = 0.22494713\n",
      "Iteration 446, loss = 0.22369843\n",
      "Iteration 447, loss = 0.22246079\n",
      "Iteration 448, loss = 0.22123412\n",
      "Iteration 449, loss = 0.22001835\n",
      "Iteration 450, loss = 0.21881339\n",
      "Iteration 451, loss = 0.21761915\n",
      "Iteration 452, loss = 0.21643556\n",
      "Iteration 453, loss = 0.21526253\n",
      "Iteration 454, loss = 0.21409997\n",
      "Iteration 455, loss = 0.21294780\n",
      "Iteration 456, loss = 0.21180593\n",
      "Iteration 457, loss = 0.21067427\n",
      "Iteration 458, loss = 0.20955274\n",
      "Iteration 459, loss = 0.20844126\n",
      "Iteration 460, loss = 0.20733973\n",
      "Iteration 461, loss = 0.20624807\n",
      "Iteration 462, loss = 0.20516619\n",
      "Iteration 463, loss = 0.20409400\n",
      "Iteration 464, loss = 0.20303142\n",
      "Iteration 465, loss = 0.20197836\n",
      "Iteration 466, loss = 0.20093474\n",
      "Iteration 467, loss = 0.19990046\n",
      "Iteration 468, loss = 0.19887544\n",
      "Iteration 469, loss = 0.19785960\n",
      "Iteration 470, loss = 0.19685284\n",
      "Iteration 471, loss = 0.19585509\n",
      "Iteration 472, loss = 0.19486626\n",
      "Iteration 473, loss = 0.19388626\n",
      "Iteration 474, loss = 0.19291501\n",
      "Iteration 475, loss = 0.19195243\n",
      "Iteration 476, loss = 0.19099842\n",
      "Iteration 477, loss = 0.19005292\n",
      "Iteration 478, loss = 0.18911584\n",
      "Iteration 479, loss = 0.18818709\n",
      "Iteration 480, loss = 0.18726659\n",
      "Iteration 481, loss = 0.18635427\n",
      "Iteration 482, loss = 0.18545004\n",
      "Iteration 483, loss = 0.18455382\n",
      "Iteration 484, loss = 0.18366554\n",
      "Iteration 485, loss = 0.18278511\n",
      "Iteration 486, loss = 0.18191247\n",
      "Iteration 487, loss = 0.18104753\n",
      "Iteration 488, loss = 0.18019021\n",
      "Iteration 489, loss = 0.17934044\n",
      "Iteration 490, loss = 0.17849814\n",
      "Iteration 491, loss = 0.17766325\n",
      "Iteration 492, loss = 0.17683569\n",
      "Iteration 493, loss = 0.17601537\n",
      "Iteration 494, loss = 0.17520224\n",
      "Iteration 495, loss = 0.17439622\n",
      "Iteration 496, loss = 0.17359724\n",
      "Iteration 497, loss = 0.17280523\n",
      "Iteration 498, loss = 0.17202012\n",
      "Iteration 499, loss = 0.17124184\n",
      "Iteration 500, loss = 0.17047032\n",
      "Iteration 501, loss = 0.16970550\n",
      "Iteration 502, loss = 0.16894731\n",
      "Iteration 503, loss = 0.16819567\n",
      "Iteration 504, loss = 0.16745054\n",
      "Iteration 505, loss = 0.16671184\n",
      "Iteration 506, loss = 0.16597951\n",
      "Iteration 507, loss = 0.16525349\n",
      "Iteration 508, loss = 0.16453371\n",
      "Iteration 509, loss = 0.16382011\n",
      "Iteration 510, loss = 0.16311263\n",
      "Iteration 511, loss = 0.16241121\n",
      "Iteration 512, loss = 0.16171579\n",
      "Iteration 513, loss = 0.16102631\n",
      "Iteration 514, loss = 0.16034271\n",
      "Iteration 515, loss = 0.15966494\n",
      "Iteration 516, loss = 0.15899293\n",
      "Iteration 517, loss = 0.15832663\n",
      "Iteration 518, loss = 0.15766599\n",
      "Iteration 519, loss = 0.15701094\n",
      "Iteration 520, loss = 0.15636144\n",
      "Iteration 521, loss = 0.15571743\n",
      "Iteration 522, loss = 0.15507885\n",
      "Iteration 523, loss = 0.15444565\n",
      "Iteration 524, loss = 0.15381778\n",
      "Iteration 525, loss = 0.15319519\n",
      "Iteration 526, loss = 0.15257782\n",
      "Iteration 527, loss = 0.15196563\n",
      "Iteration 528, loss = 0.15135856\n",
      "Iteration 529, loss = 0.15075657\n",
      "Iteration 530, loss = 0.15015960\n",
      "Iteration 531, loss = 0.14956760\n",
      "Iteration 532, loss = 0.14898053\n",
      "Iteration 533, loss = 0.14839834\n",
      "Iteration 534, loss = 0.14782098\n",
      "Iteration 535, loss = 0.14724841\n",
      "Iteration 536, loss = 0.14668057\n",
      "Iteration 537, loss = 0.14611743\n",
      "Iteration 538, loss = 0.14555893\n",
      "Iteration 539, loss = 0.14500504\n",
      "Iteration 540, loss = 0.14445570\n",
      "Iteration 541, loss = 0.14391087\n",
      "Iteration 542, loss = 0.14337051\n",
      "Iteration 543, loss = 0.14283457\n",
      "Iteration 544, loss = 0.14230302\n",
      "Iteration 545, loss = 0.14177581\n",
      "Iteration 546, loss = 0.14125289\n",
      "Iteration 547, loss = 0.14073423\n",
      "Iteration 548, loss = 0.14021978\n",
      "Iteration 549, loss = 0.13970950\n",
      "Iteration 550, loss = 0.13920336\n",
      "Iteration 551, loss = 0.13870131\n",
      "Iteration 552, loss = 0.13820331\n",
      "Iteration 553, loss = 0.13770932\n",
      "Iteration 554, loss = 0.13721931\n",
      "Iteration 555, loss = 0.13673323\n",
      "Iteration 556, loss = 0.13625105\n",
      "Iteration 557, loss = 0.13577273\n",
      "Iteration 558, loss = 0.13529823\n",
      "Iteration 559, loss = 0.13482751\n",
      "Iteration 560, loss = 0.13436053\n",
      "Iteration 561, loss = 0.13389727\n",
      "Iteration 562, loss = 0.13343768\n",
      "Iteration 563, loss = 0.13298173\n",
      "Iteration 564, loss = 0.13252938\n",
      "Iteration 565, loss = 0.13208060\n",
      "Iteration 566, loss = 0.13163535\n",
      "Iteration 567, loss = 0.13119360\n",
      "Iteration 568, loss = 0.13075531\n",
      "Iteration 569, loss = 0.13032044\n",
      "Iteration 570, loss = 0.12988898\n",
      "Iteration 571, loss = 0.12946087\n",
      "Iteration 572, loss = 0.12903610\n",
      "Iteration 573, loss = 0.12861462\n",
      "Iteration 574, loss = 0.12819640\n",
      "Iteration 575, loss = 0.12778142\n",
      "Iteration 576, loss = 0.12736964\n",
      "Iteration 577, loss = 0.12696102\n",
      "Iteration 578, loss = 0.12655554\n",
      "Iteration 579, loss = 0.12615317\n",
      "Iteration 580, loss = 0.12575388\n",
      "Iteration 581, loss = 0.12535763\n",
      "Iteration 582, loss = 0.12496439\n",
      "Iteration 583, loss = 0.12457414\n",
      "Iteration 584, loss = 0.12418685\n",
      "Iteration 585, loss = 0.12380249\n",
      "Iteration 586, loss = 0.12342102\n",
      "Iteration 587, loss = 0.12304243\n",
      "Iteration 588, loss = 0.12266667\n",
      "Iteration 589, loss = 0.12229373\n",
      "Iteration 590, loss = 0.12192358\n",
      "Iteration 591, loss = 0.12155618\n",
      "Iteration 592, loss = 0.12119152\n",
      "Iteration 593, loss = 0.12082956\n",
      "Iteration 594, loss = 0.12047028\n",
      "Iteration 595, loss = 0.12011365\n",
      "Iteration 596, loss = 0.11975965\n",
      "Iteration 597, loss = 0.11940825\n",
      "Iteration 598, loss = 0.11905942\n",
      "Iteration 599, loss = 0.11871314\n",
      "Iteration 600, loss = 0.11836939\n",
      "Iteration 601, loss = 0.11802814\n",
      "Iteration 602, loss = 0.11768936\n",
      "Iteration 603, loss = 0.11735304\n",
      "Iteration 604, loss = 0.11701914\n",
      "Iteration 605, loss = 0.11668764\n",
      "Iteration 606, loss = 0.11635853\n",
      "Iteration 607, loss = 0.11603177\n",
      "Iteration 608, loss = 0.11570735\n",
      "Iteration 609, loss = 0.11538524\n",
      "Iteration 610, loss = 0.11506542\n",
      "Iteration 611, loss = 0.11474786\n",
      "Iteration 612, loss = 0.11443256\n",
      "Iteration 613, loss = 0.11411947\n",
      "Iteration 614, loss = 0.11380859\n",
      "Iteration 615, loss = 0.11349988\n",
      "Iteration 616, loss = 0.11319334\n",
      "Iteration 617, loss = 0.11288894\n",
      "Iteration 618, loss = 0.11258665\n",
      "Iteration 619, loss = 0.11228646\n",
      "Iteration 620, loss = 0.11198836\n",
      "Iteration 621, loss = 0.11169231\n",
      "Iteration 622, loss = 0.11139829\n",
      "Iteration 623, loss = 0.11110630\n",
      "Iteration 624, loss = 0.11081631\n",
      "Iteration 625, loss = 0.11052830\n",
      "Iteration 626, loss = 0.11024225\n",
      "Iteration 627, loss = 0.10995815\n",
      "Iteration 628, loss = 0.10967597\n",
      "Iteration 629, loss = 0.10939569\n",
      "Iteration 630, loss = 0.10911731\n",
      "Iteration 631, loss = 0.10884080\n",
      "Iteration 632, loss = 0.10856614\n",
      "Iteration 633, loss = 0.10829332\n",
      "Iteration 634, loss = 0.10802232\n",
      "Iteration 635, loss = 0.10775313\n",
      "Iteration 636, loss = 0.10748572\n",
      "Iteration 637, loss = 0.10722007\n",
      "Iteration 638, loss = 0.10695618\n",
      "Iteration 639, loss = 0.10669403\n",
      "Iteration 640, loss = 0.10643360\n",
      "Iteration 641, loss = 0.10617487\n",
      "Iteration 642, loss = 0.10591783\n",
      "Iteration 643, loss = 0.10566247\n",
      "Iteration 644, loss = 0.10540876\n",
      "Iteration 645, loss = 0.10515670\n",
      "Iteration 646, loss = 0.10490627\n",
      "Iteration 647, loss = 0.10465745\n",
      "Iteration 648, loss = 0.10441022\n",
      "Iteration 649, loss = 0.10416459\n",
      "Iteration 650, loss = 0.10392052\n",
      "Iteration 651, loss = 0.10367801\n",
      "Iteration 652, loss = 0.10343704\n",
      "Iteration 653, loss = 0.10319760\n",
      "Iteration 654, loss = 0.10295967\n",
      "Iteration 655, loss = 0.10272324\n",
      "Iteration 656, loss = 0.10248831\n",
      "Iteration 657, loss = 0.10225484\n",
      "Iteration 658, loss = 0.10202284\n",
      "Iteration 659, loss = 0.10179228\n",
      "Iteration 660, loss = 0.10156317\n",
      "Iteration 661, loss = 0.10133547\n",
      "Iteration 662, loss = 0.10110918\n",
      "Iteration 663, loss = 0.10088430\n",
      "Iteration 664, loss = 0.10066080\n",
      "Iteration 665, loss = 0.10043867\n",
      "Iteration 666, loss = 0.10021790\n",
      "Iteration 667, loss = 0.09999849\n",
      "Iteration 668, loss = 0.09978041\n",
      "Iteration 669, loss = 0.09956366\n",
      "Iteration 670, loss = 0.09934823\n",
      "Iteration 671, loss = 0.09913410\n",
      "Iteration 672, loss = 0.09892126\n",
      "Iteration 673, loss = 0.09870970\n",
      "Iteration 674, loss = 0.09849942\n",
      "Iteration 675, loss = 0.09829039\n",
      "Iteration 676, loss = 0.09808262\n",
      "Iteration 677, loss = 0.09787608\n",
      "Iteration 678, loss = 0.09767077\n",
      "Iteration 679, loss = 0.09746668\n",
      "Iteration 680, loss = 0.09726380\n",
      "Iteration 681, loss = 0.09706212\n",
      "Iteration 682, loss = 0.09686162\n",
      "Iteration 683, loss = 0.09666230\n",
      "Iteration 684, loss = 0.09646415\n",
      "Iteration 685, loss = 0.09626716\n",
      "Iteration 686, loss = 0.09607131\n",
      "Iteration 687, loss = 0.09587661\n",
      "Iteration 688, loss = 0.09568304\n",
      "Iteration 689, loss = 0.09549058\n",
      "Iteration 690, loss = 0.09529924\n",
      "Iteration 691, loss = 0.09510900\n",
      "Iteration 692, loss = 0.09491985\n",
      "Iteration 693, loss = 0.09473179\n",
      "Iteration 694, loss = 0.09454481\n",
      "Iteration 695, loss = 0.09435889\n",
      "Iteration 696, loss = 0.09417403\n",
      "Iteration 697, loss = 0.09399022\n",
      "Iteration 698, loss = 0.09380745\n",
      "Iteration 699, loss = 0.09362571\n",
      "Iteration 700, loss = 0.09344500\n",
      "Iteration 701, loss = 0.09326530\n",
      "Iteration 702, loss = 0.09308662\n",
      "Iteration 703, loss = 0.09290893\n",
      "Iteration 704, loss = 0.09273224\n",
      "Iteration 705, loss = 0.09255653\n",
      "Iteration 706, loss = 0.09238180\n",
      "Iteration 707, loss = 0.09220804\n",
      "Iteration 708, loss = 0.09203524\n",
      "Iteration 709, loss = 0.09186340\n",
      "Iteration 710, loss = 0.09169250\n",
      "Iteration 711, loss = 0.09152255\n",
      "Iteration 712, loss = 0.09135352\n",
      "Iteration 713, loss = 0.09118542\n",
      "Iteration 714, loss = 0.09101824\n",
      "Iteration 715, loss = 0.09085197\n",
      "Iteration 716, loss = 0.09068660\n",
      "Iteration 717, loss = 0.09052213\n",
      "Iteration 718, loss = 0.09035856\n",
      "Iteration 719, loss = 0.09019586\n",
      "Iteration 720, loss = 0.09003404\n",
      "Iteration 721, loss = 0.08987309\n",
      "Iteration 722, loss = 0.08971301\n",
      "Iteration 723, loss = 0.08955378\n",
      "Iteration 724, loss = 0.08939540\n",
      "Iteration 725, loss = 0.08923787\n",
      "Iteration 726, loss = 0.08908118\n",
      "Iteration 727, loss = 0.08892531\n",
      "Iteration 728, loss = 0.08877028\n",
      "Iteration 729, loss = 0.08861606\n",
      "Iteration 730, loss = 0.08846265\n",
      "Iteration 731, loss = 0.08831006\n",
      "Iteration 732, loss = 0.08815826\n",
      "Iteration 733, loss = 0.08800726\n",
      "Iteration 734, loss = 0.08785705\n",
      "Iteration 735, loss = 0.08770763\n",
      "Iteration 736, loss = 0.08755898\n",
      "Iteration 737, loss = 0.08741110\n",
      "Iteration 738, loss = 0.08726399\n",
      "Iteration 739, loss = 0.08711765\n",
      "Iteration 740, loss = 0.08697206\n",
      "Iteration 741, loss = 0.08682722\n",
      "Iteration 742, loss = 0.08668312\n",
      "Iteration 743, loss = 0.08653976\n",
      "Iteration 744, loss = 0.08639714\n",
      "Iteration 745, loss = 0.08625525\n",
      "Iteration 746, loss = 0.08611408\n",
      "Iteration 747, loss = 0.08597363\n",
      "Iteration 748, loss = 0.08583390\n",
      "Iteration 749, loss = 0.08569487\n",
      "Iteration 750, loss = 0.08555655\n",
      "Iteration 751, loss = 0.08541893\n",
      "Iteration 752, loss = 0.08528200\n",
      "Iteration 753, loss = 0.08514576\n",
      "Iteration 754, loss = 0.08501020\n",
      "Iteration 755, loss = 0.08487532\n",
      "Iteration 756, loss = 0.08474112\n",
      "Iteration 757, loss = 0.08460759\n",
      "Iteration 758, loss = 0.08447472\n",
      "Iteration 759, loss = 0.08434251\n",
      "Iteration 760, loss = 0.08421096\n",
      "Iteration 761, loss = 0.08408007\n",
      "Iteration 762, loss = 0.08394982\n",
      "Iteration 763, loss = 0.08382021\n",
      "Iteration 764, loss = 0.08369124\n",
      "Iteration 765, loss = 0.08356290\n",
      "Iteration 766, loss = 0.08343520\n",
      "Iteration 767, loss = 0.08330812\n",
      "Iteration 768, loss = 0.08318166\n",
      "Iteration 769, loss = 0.08305582\n",
      "Iteration 770, loss = 0.08293059\n",
      "Iteration 771, loss = 0.08280597\n",
      "Iteration 772, loss = 0.08268196\n",
      "Iteration 773, loss = 0.08255854\n",
      "Iteration 774, loss = 0.08243573\n",
      "Iteration 775, loss = 0.08231350\n",
      "Iteration 776, loss = 0.08219187\n",
      "Iteration 777, loss = 0.08207082\n",
      "Iteration 778, loss = 0.08195035\n",
      "Iteration 779, loss = 0.08183046\n",
      "Iteration 780, loss = 0.08171114\n",
      "Iteration 781, loss = 0.08159239\n",
      "Iteration 782, loss = 0.08147421\n",
      "Iteration 783, loss = 0.08135659\n",
      "Iteration 784, loss = 0.08123952\n",
      "Iteration 785, loss = 0.08112302\n",
      "Iteration 786, loss = 0.08100706\n",
      "Iteration 787, loss = 0.08089165\n",
      "Iteration 788, loss = 0.08077679\n",
      "Iteration 789, loss = 0.08066247\n",
      "Iteration 790, loss = 0.08054868\n",
      "Iteration 791, loss = 0.08043543\n",
      "Iteration 792, loss = 0.08032271\n",
      "Iteration 793, loss = 0.08021051\n",
      "Iteration 794, loss = 0.08009884\n",
      "Iteration 795, loss = 0.07998769\n",
      "Iteration 796, loss = 0.07987705\n",
      "Iteration 797, loss = 0.07976693\n",
      "Iteration 798, loss = 0.07965732\n",
      "Iteration 799, loss = 0.07954822\n",
      "Iteration 800, loss = 0.07943962\n",
      "Iteration 801, loss = 0.07933152\n",
      "Iteration 802, loss = 0.07922392\n",
      "Iteration 803, loss = 0.07911682\n",
      "Iteration 804, loss = 0.07901020\n",
      "Iteration 805, loss = 0.07890407\n",
      "Iteration 806, loss = 0.07879843\n",
      "Iteration 807, loss = 0.07869327\n",
      "Iteration 808, loss = 0.07858859\n",
      "Iteration 809, loss = 0.07848439\n",
      "Iteration 810, loss = 0.07838066\n",
      "Iteration 811, loss = 0.07827740\n",
      "Iteration 812, loss = 0.07817461\n",
      "Iteration 813, loss = 0.07807228\n",
      "Iteration 814, loss = 0.07797042\n",
      "Iteration 815, loss = 0.07786901\n",
      "Iteration 816, loss = 0.07776806\n",
      "Iteration 817, loss = 0.07766756\n",
      "Iteration 818, loss = 0.07756751\n",
      "Iteration 819, loss = 0.07746792\n",
      "Iteration 820, loss = 0.07736876\n",
      "Iteration 821, loss = 0.07727005\n",
      "Iteration 822, loss = 0.07717178\n",
      "Iteration 823, loss = 0.07707395\n",
      "Iteration 824, loss = 0.07697655\n",
      "Iteration 825, loss = 0.07687958\n",
      "Iteration 826, loss = 0.07678304\n",
      "Iteration 827, loss = 0.07668693\n",
      "Iteration 828, loss = 0.07659124\n",
      "Iteration 829, loss = 0.07649597\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(10, 8), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16  0  0]\n",
      " [ 0  9  1]\n",
      " [ 0  0 12]]\n"
     ]
    }
   ],
   "source": [
    "predictions = mlp.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "cf = confusion_matrix(y_test,predictions)\n",
    "print(cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9736842105263158"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "akurasi = (cf[0][0] + cf[1][1] + cf[2][2]) / cf.sum()\n",
    "akurasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nullphantom/miniconda3/envs/ml/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "from sklearn.externals import joblib\n",
    "filename = 'iris_model.sav'\n",
    "joblib.dump(mlp, filename)\n",
    "print(\"model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [5.5, 2.5, 5, 1.3] #????\n",
    "# data = [3.2, 1.6, 1.4, 0.5] #setosa\n",
    "# data = [5.1, 3.5, 1.4, 0.2] #setosa\n",
    "# data = [5.9, 2.2, 4.0, 1.5] #versicolor\n",
    "# data = [5.4, 2.2, 4.2, 1.2] #versicolor\n",
    "# data = [6.0, 3.5, 5.4, 2.2] #virginica\n",
    "# data = [5.8, 3.1, 5.0, 1.7] #virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = mlp.predict([data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n"
     ]
    }
   ],
   "source": [
    "print (prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
